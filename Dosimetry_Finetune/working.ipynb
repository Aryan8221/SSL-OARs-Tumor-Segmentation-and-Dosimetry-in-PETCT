{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-09T18:54:00.810608Z",
     "start_time": "2024-07-09T18:53:47.765646Z"
    }
   },
   "source": [
    "from monai import data, transforms\n",
    "import os\n",
    "from monai.data import load_decathlon_datalist\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.multiprocessing import set_start_method\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"TypedStorage is deprecated\")\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T18:54:00.819052Z",
     "start_time": "2024-07-09T18:54:00.812068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True, make_even=True):\n",
    "        if num_replicas is None:\n",
    "            if not torch.distributed.is_available():\n",
    "                raise RuntimeError(\"Requires distributed package to be available\")\n",
    "            num_replicas = torch.distributed.get_world_size()\n",
    "        if rank is None:\n",
    "            if not torch.distributed.is_available():\n",
    "                raise RuntimeError(\"Requires distributed package to be available\")\n",
    "            rank = torch.distributed.get_rank()\n",
    "        self.shuffle = shuffle\n",
    "        self.make_even = make_even\n",
    "        self.dataset = dataset\n",
    "        self.num_replicas = num_replicas\n",
    "        self.rank = rank\n",
    "        self.epoch = 0\n",
    "        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n",
    "        self.total_size = self.num_samples * self.num_replicas\n",
    "        indices = list(range(len(self.dataset)))\n",
    "        self.valid_length = len(indices[self.rank : self.total_size : self.num_replicas])\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            g = torch.Generator()\n",
    "            g.manual_seed(self.epoch)\n",
    "            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n",
    "        else:\n",
    "            indices = list(range(len(self.dataset)))\n",
    "        if self.make_even:\n",
    "            if len(indices) < self.total_size:\n",
    "                if self.total_size - len(indices) < len(indices):\n",
    "                    indices += indices[: (self.total_size - len(indices))]\n",
    "                else:\n",
    "                    extra_ids = np.random.randint(low=0, high=len(indices), size=self.total_size - len(indices))\n",
    "                    indices += [indices[ids] for ids in extra_ids]\n",
    "            assert len(indices) == self.total_size\n",
    "        indices = indices[self.rank : self.total_size : self.num_replicas]\n",
    "        self.num_samples = len(indices)\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n"
   ],
   "id": "58800ce459a963cc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:06:37.345293Z",
     "start_time": "2024-07-09T19:06:37.329872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from monai import data, transforms\n",
    "from monai.data import load_decathlon_datalist\n",
    "from monai.transforms import RandSpatialCropd\n",
    "\n",
    "\n",
    "# normalized form \n",
    "def get_loader():\n",
    "    \n",
    "    data_dir = '/Users/aryanneizehbaz/Aryan8221/coding_projects/SSL-OARs-Tumor-Segmentation-in-PETCT/Dosimetry_Finetune/aspect_data'\n",
    "    # data_dir = '/Users/aryanneizehbaz/Aryan8221/coding_projects/SSL-OARs-Tumor-Segmentation-in-PETCT/Finetune/masked_data'\n",
    "    \n",
    "    datalist_json = os.path.join(data_dir, 'fold0.json')\n",
    "    # datalist_json = os.path.join(data_dir, 'masked_data2.json')\n",
    "    \n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\", \"label\"], image_only=False),\n",
    "            transforms.EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "            transforms.Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "            transforms.CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "            RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=(96, 96, 96), random_size=False),\n",
    "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.2, spatial_axis=0),\n",
    "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.2, spatial_axis=1),\n",
    "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.2, spatial_axis=2),\n",
    "            transforms.RandRotate90d(keys=[\"image\", \"label\"], prob=0.2, max_k=3),\n",
    "            transforms.RandScaleIntensityd(keys=\"image\", factors=0.1, prob=0.1),\n",
    "            transforms.RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=0.1),\n",
    "            transforms.ToTensord(keys=[\"image\", \"label\"]),\n",
    "        ]\n",
    "    )\n",
    "    val_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\", \"label\"],image_only=False),\n",
    "            transforms.EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "            transforms.Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "            transforms.CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "            transforms.ToTensord(keys=[\"image\", \"label\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\", \"label\"],image_only=True),\n",
    "            transforms.EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "            # transforms.Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "            transforms.ToTensord(keys=[\"image\", \"label\"]),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    test_mode = False\n",
    "    distributed = False\n",
    "    use_normal_dataset = False\n",
    "    \n",
    "    if test_mode:\n",
    "        test_files = load_decathlon_datalist(datalist_json, True, \"validation\", base_dir=data_dir)\n",
    "        test_ds = data.Dataset(data=test_files, transform=test_transform)\n",
    "        test_sampler = Sampler(test_ds, shuffle=False) if distributed else None\n",
    "        test_loader = data.DataLoader(\n",
    "            test_ds,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            sampler=test_sampler,\n",
    "            pin_memory=False,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "        loader = test_loader\n",
    "    else:\n",
    "        datalist = load_decathlon_datalist(datalist_json, True, \"training\", base_dir=data_dir)\n",
    "        if use_normal_dataset:\n",
    "            train_ds = data.Dataset(data=datalist, transform=train_transform)\n",
    "        else:\n",
    "            train_ds = data.CacheDataset(\n",
    "                data=datalist, transform=train_transform, cache_num=24, cache_rate=1.0, num_workers=4\n",
    "            )\n",
    "        train_sampler = Sampler(train_ds) if distributed else None\n",
    "        train_loader = data.DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=1,\n",
    "            shuffle=(train_sampler is None),\n",
    "            num_workers=4,\n",
    "            sampler=train_sampler,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "        val_files = load_decathlon_datalist(datalist_json, True, \"validation\", base_dir=data_dir)\n",
    "        val_ds = data.Dataset(data=val_files, transform=val_transform)\n",
    "        val_sampler = Sampler(val_ds, shuffle=False) if distributed else None\n",
    "        val_loader = data.DataLoader(\n",
    "            val_ds, batch_size=1, shuffle=False, num_workers=4, sampler=val_sampler, pin_memory=False\n",
    "        )\n",
    "        loader = [train_loader, val_loader]\n",
    "\n",
    "    return loader\n"
   ],
   "id": "c6b00d6c5835ba0b",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:07:09.512952Z",
     "start_time": "2024-07-09T19:06:37.429638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = get_loader()\n",
    "\n",
    "print(len(loader))\n",
    "train = []\n",
    "val = []\n",
    "\n",
    "# for i, batch in enumerate(loader):\n",
    "#     print(batch)\n",
    "\n",
    "for i, batch in enumerate(loader[0]):\n",
    "    train.append(batch)\n",
    "\n",
    "for i, batch in enumerate(loader[1]):\n",
    "    val.append(batch) "
   ],
   "id": "defefd829801c9d1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 19/19 [00:06<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/monai/data/__init__.py:117: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  t = cls([], dtype=storage.dtype, device=storage.device)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/monai/data/__init__.py:117: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  t = cls([], dtype=storage.dtype, device=storage.device)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/monai/data/__init__.py:117: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  t = cls([], dtype=storage.dtype, device=storage.device)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/monai/data/__init__.py:117: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  t = cls([], dtype=storage.dtype, device=storage.device)\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T18:55:35.856552Z",
     "start_time": "2024-07-09T18:55:35.852830Z"
    }
   },
   "cell_type": "code",
   "source": "len(train)",
   "id": "e32583bcdfed735f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:07:09.530331Z",
     "start_time": "2024-07-09T19:07:09.519731Z"
    }
   },
   "cell_type": "code",
   "source": "tensor_data = train[0]['label'] == 0",
   "id": "54070d0431e3a707",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:07:09.541662Z",
     "start_time": "2024-07-09T19:07:09.533411Z"
    }
   },
   "cell_type": "code",
   "source": "(tensor_data == True).sum().item()",
   "id": "a52fd7392a5ea53f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8036"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T13:31:31.455423Z",
     "start_time": "2024-07-06T13:31:22.328322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "for i in range(13):\n",
    "    print(train[i][\"label_meta_dict\"][\"filename_or_obj\"])\n",
    "    print(np.unique(train[i]['label']))\n",
    "    print('-' * 10)"
   ],
   "id": "3fb62f8cd8248d95",
   "execution_count": 35,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T13:09:04.304024Z",
     "start_time": "2024-07-06T13:09:04.272077Z"
    }
   },
   "cell_type": "code",
   "source": "train[0][\"label_meta_dict\"][\"filename_or_obj\"]",
   "id": "60c80aa63fb305ab",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T12:57:35.199152Z",
     "start_time": "2024-07-06T12:57:35.110550Z"
    }
   },
   "cell_type": "code",
   "source": "np.unique(train[0]['label'])",
   "id": "58e9c3b383f20307",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T15:29:27.844325Z",
     "start_time": "2024-07-05T15:29:27.818530Z"
    }
   },
   "cell_type": "code",
   "source": "train[0]['label_meta_dict'][\"affine\"]",
   "id": "1f9844844f1b1b37",
   "execution_count": 36,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T15:29:39.693840Z",
     "start_time": "2024-07-05T15:29:39.669597Z"
    }
   },
   "cell_type": "code",
   "source": "train[0]['image_meta_dict'][\"affine\"]",
   "id": "754a0d18ce7f63",
   "execution_count": 37,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T18:55:24.070861Z",
     "start_time": "2024-07-09T18:55:23.905172Z"
    }
   },
   "cell_type": "code",
   "source": "torch.unique(train[0]['label'].flatten().sort()[0])",
   "id": "ed994f84625ff9e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.0748e-05, 1.0858e-04,  ..., 7.8317e+02, 9.1843e+02,\n",
       "        9.4709e+02])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T20:35:44.470861Z",
     "start_time": "2024-06-28T20:35:44.355725Z"
    }
   },
   "cell_type": "code",
   "source": "torch.unique(train[0]['image'].flatten().sort()[0])",
   "id": "cb9a65e0b66130c8",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:00:38.778317Z",
     "start_time": "2024-06-12T20:00:38.744164Z"
    }
   },
   "cell_type": "code",
   "source": "val[0]['image'].shape",
   "id": "b02b35c931a3d2f5",
   "execution_count": 55,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:38:47.479377Z",
     "start_time": "2024-06-12T20:38:47.358866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assume these were the min and max values used during normalization\n",
    "ct_hu_min, ct_hu_max = -200, 1000\n",
    "pet_hu_min, pet_hu_max = 0, 100\n",
    "\n",
    "# Reverse normalization function\n",
    "def reverse_normalize(data, min_val, max_val):\n",
    "    return data * (max_val - min_val) + min_val\n",
    "\n",
    "# Get the first batch\n",
    "image = train[0]['image']\n",
    "label = train[0]['label']\n",
    "\n",
    "# Reverse normalize the image and label\n",
    "image_original = reverse_normalize(image.numpy(), ct_hu_min, ct_hu_max)\n",
    "label_original = reverse_normalize(label.numpy(), pet_hu_min, pet_hu_max)\n",
    "\n",
    "print(\"Original Image Mean:\", image_original.flatten().mean())\n",
    "print(\"Original Label Mean:\", label_original.flatten().mean())"
   ],
   "id": "f7f39e5d97904686",
   "execution_count": 64,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:46:30.296420Z",
     "start_time": "2024-06-12T20:46:30.284292Z"
    }
   },
   "cell_type": "code",
   "source": "len(label_original.flatten())",
   "id": "3b374d8d454a1a79",
   "execution_count": 72,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T21:24:11.183674Z",
     "start_time": "2024-06-12T21:24:11.166430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_zeros = np.sum(label_original.flatten() < 20)\n",
    "print(\"Number of zeros in the label:\", num_zeros)"
   ],
   "id": "5915ade4884e56a9",
   "execution_count": 98,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:49:25.241800Z",
     "start_time": "2024-06-12T20:49:19.621108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Flatten the label array\n",
    "label_flattened = label_original.flatten()\n",
    "\n",
    "# Draw the histogram\n",
    "plt.hist(label_flattened, bins=5000, edgecolor='black')\n",
    "plt.title('Histogram of label_original values')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "3d8245d8e4ecf51c",
   "execution_count": 78,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:46:05.127471Z",
     "start_time": "2024-06-12T20:46:04.766768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Find unique values and sort them in ascending order\n",
    "unique_image_values = np.unique(image_original)\n",
    "unique_label_values = np.unique(label_original)\n",
    "\n",
    "# Print unique values\n",
    "print(\"Unique Image Values (ascending):\", unique_image_values)\n",
    "print(\"Unique Label Values (ascending):\", unique_label_values)"
   ],
   "id": "c8ab2cfac22c800a",
   "execution_count": 71,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T21:30:32.804220Z",
     "start_time": "2024-06-12T21:30:32.794157Z"
    }
   },
   "cell_type": "code",
   "source": "len(unique_label_values)",
   "id": "99d80c818eb4978f",
   "execution_count": 110,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T21:25:38.315428Z",
     "start_time": "2024-06-12T21:25:38.272724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.array([23, 53, 63, 45, 23, 57, 75, 100, 23, 34])\n",
    "y_pred = np.array([20, 23, 50, 30, 23, 13, 75, 78, 24, 99])\n",
    "\n",
    "# Flatten the arrays to ensure element-wise operations\n",
    "y_true = y_true.flatten()\n",
    "y_pred = y_pred.flatten()\n",
    "\n",
    "# Filter out zero values in y_true\n",
    "mask = y_true != 0\n",
    "y_true = y_true[mask]\n",
    "y_pred = y_pred[mask]\n",
    "\n",
    "# Calculate the numerator and denominator\n",
    "numerator = np.abs(y_true - y_pred)\n",
    "denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "\n",
    "\n",
    "# Calculate sMAPE\n",
    "smape = np.mean(2.0 * numerator / denominator) * 100\n",
    "\n",
    "print(\"sMAPE:\", smape)\n"
   ],
   "id": "47491316f4f925c7",
   "execution_count": 105,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T21:25:40.445951Z",
     "start_time": "2024-06-12T21:25:40.425965Z"
    }
   },
   "cell_type": "code",
   "source": "numerator",
   "id": "efecdeaef08486f3",
   "execution_count": 106,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T21:25:41.065415Z",
     "start_time": "2024-06-12T21:25:41.032722Z"
    }
   },
   "cell_type": "code",
   "source": "denominator",
   "id": "ce185ac38f710667",
   "execution_count": 107,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T15:31:12.297316Z",
     "start_time": "2024-06-06T15:31:07.951298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Print the affine matrices for the first batch\n",
    "for i, batch in enumerate(loader):\n",
    "    for j in range(len(batch[\"image\"])):\n",
    "        print(f\"Batch {i}, Image {j} Affine:\\n\", batch[\"image_meta_dict\"][\"affine\"][j])\n",
    "        print(f\"Batch {i}, Label {j} Affine:\\n\", batch[\"label_meta_dict\"][\"affine\"][j])\n",
    "    if i == 0:  # Just print the first batch for brevity\n",
    "        break"
   ],
   "id": "bc854b26fb670afa",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T13:41:44.239909Z",
     "start_time": "2024-06-06T13:41:44.232473Z"
    }
   },
   "cell_type": "code",
   "source": "print(batches[0].keys())",
   "id": "cfea15cdbf9260e4",
   "execution_count": 147,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T17:53:57.776566Z",
     "start_time": "2024-06-05T17:53:57.750053Z"
    }
   },
   "cell_type": "code",
   "source": "batches[0]['image_meta_dict']",
   "id": "bc49300462613f89",
   "execution_count": 119,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T16:54:36.462724Z",
     "start_time": "2024-06-05T16:54:04.341250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, batch in enumerate(loader[1]):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    for key in batch.keys():\n",
    "        print(f\"Key: {key}\")\n",
    "        if isinstance(batch[key], dict):\n",
    "            for meta_key in batch[key].keys():\n",
    "                print(f\"  Meta Key: {meta_key}\")\n",
    "                if isinstance(batch[key][meta_key], dict):\n",
    "                    for sub_meta_key in batch[key][meta_key].keys():\n",
    "                        print(f\"    Sub Meta Key: {sub_meta_key}\")\n",
    "    break  # Only process the first batch for demonstration"
   ],
   "id": "34ea18bb72c17495",
   "execution_count": 108,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T19:28:19.618164Z",
     "start_time": "2024-06-06T19:28:19.520059Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2bccfaa12f30b424",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "def extract_losses(log_file_path):\n",
    "    loss_pattern = re.compile(r'Final training\\s+\\d+/\\d+\\s+loss:\\s+([0-9]+\\.[0-9]+)')\n",
    "    losses = []\n",
    "    \n",
    "    with open(log_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            match = loss_pattern.search(line)\n",
    "            if match:\n",
    "                losses.append(float(match.group(1)))\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def plot_losses(losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses, label=\"Training Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "log_file_path = \"/Users/aryanneizehbaz/Aryan8221/coding_projects/SSL-OARs-Tumor-Segmentation-in-PETCT/Dosimetry_Finetune/runs/logFile_2024-06-06_21:20.log\"\n",
    "\n",
    "losses = extract_losses(log_file_path)\n",
    "\n",
    "plot_losses(losses)\n"
   ],
   "id": "df601ad05b2aea67",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:38:29.606860Z",
     "start_time": "2024-06-13T19:38:29.265536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def extract_epoch_mse_losses(log_file_path):\n",
    "    epoch_pattern = re.compile(r'Epoch (\\d+)/\\d+ \\d+/\\d+\\s+loss:\\s*([0-9]+\\.[0-9]+)')\n",
    "    epoch_losses = {}\n",
    "    \n",
    "    with open(log_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            match = epoch_pattern.search(line)\n",
    "            if match:\n",
    "                epoch = int(match.group(1))\n",
    "                loss = float(match.group(2))\n",
    "                if epoch not in epoch_losses:\n",
    "                    epoch_losses[epoch] = []\n",
    "                epoch_losses[epoch].append(loss)\n",
    "    \n",
    "    avg_epoch_losses = [sum(losses) / len(losses) for epoch, losses in sorted(epoch_losses.items())]\n",
    "    \n",
    "    return avg_epoch_losses\n",
    "\n",
    "def smooth_losses(losses, window_size=10):\n",
    "    return np.convolve(losses, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "def plot_avg_epoch_mse_losses(avg_epoch_losses, smoothed_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(avg_epoch_losses, marker='o', linestyle='-', alpha=0.5, label=\"Original Average Epoch MSE Loss\")\n",
    "    plt.plot(smoothed_losses, marker='', linestyle='-', label=\"Smoothed Average Epoch MSE Loss\", linewidth=2)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Average MSE Loss\")\n",
    "    plt.title(\"Average MSE Loss per Epoch\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "log_file_path = \"/Users/aryanneizehbaz/Aryan8221/coding_projects/SSL-OARs-Tumor-Segmentation-in-PETCT/Dosimetry_Finetune/runs/pet-fold0.log\"\n",
    "\n",
    "avg_epoch_losses = extract_epoch_mse_losses(log_file_path)\n",
    "\n",
    "smoothed_losses = smooth_losses(avg_epoch_losses, window_size=10)\n",
    "\n",
    "plot_avg_epoch_mse_losses(avg_epoch_losses, smoothed_losses)\n"
   ],
   "id": "cd01adbcebb407e6",
   "execution_count": 120,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T16:51:39.239008Z",
     "start_time": "2024-07-03T16:51:39.157464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    Orientationd,\n",
    "    CropForegroundd,\n",
    "    ToTensord,\n",
    "    Compose,\n",
    ")\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from utils.data_utils import get_loader\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "def prepare_data(data_dir, json_list, roi_size):\n",
    "    datalist_json = os.path.join(data_dir, json_list)\n",
    "    with open(datalist_json) as f:\n",
    "        datalist = json.load(f)\n",
    "    data = [{\"image\": item[\"image\"], \"label\": item[\"label\"]} for item in datalist[\"validation\"]]\n",
    "\n",
    "    val_transform = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"image\", \"label\"], image_only=True),\n",
    "            EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "            Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "            CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "            ToTensord(keys=[\"image\", \"label\"]),\n",
    "        ]\n",
    "    )\n",
    "    dataset = Dataset(data=data, transform=val_transform)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "    return loader\n"
   ],
   "id": "77e9954a3c0981db",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T16:02:13.712053Z",
     "start_time": "2024-07-03T16:02:13.686097Z"
    }
   },
   "cell_type": "code",
   "source": "loader",
   "id": "2731d57bbfec0384",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T16:03:00.528154Z",
     "start_time": "2024-07-03T16:03:00.509308Z"
    }
   },
   "cell_type": "code",
   "source": "len(loader)",
   "id": "d105477aa5035c47",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T16:02:50.024184Z",
     "start_time": "2024-07-03T16:02:50.009321Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b9d6a6372104a615",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "a115a0e8213c062d",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
